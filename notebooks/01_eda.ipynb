{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis (EDA) - Heart Disease Dataset\n",
    "\n",
    "**Team Member:** Lam Nguyen (Lead)  \n",
    "**Course:** CMPE 257 - Machine Learning  \n",
    "**Project:** Heart Disease Risk Assessment - Multi-Class Prediction\n",
    "\n",
    "---\n",
    "\n",
    "## üìö What is EDA and Why Do We Need It?\n",
    "\n",
    "**Exploratory Data Analysis (EDA)** is the critical first step in any machine learning project. Think of it as \"getting to know your data\" before building models.\n",
    "\n",
    "### Purpose of EDA:\n",
    "1. **Understand the data structure** - What features do we have? What do they mean?\n",
    "2. **Identify data quality issues** - Missing values? Outliers? Errors?\n",
    "3. **Discover patterns and relationships** - Which features are correlated? Which might be important?\n",
    "4. **Detect problems early** - Class imbalance? Data distribution issues?\n",
    "5. **Inform preprocessing decisions** - What transformations do we need?\n",
    "\n",
    "### Why EDA Matters:\n",
    "- **Garbage in, garbage out** - If we don't understand our data, we'll build bad models\n",
    "- **Prevents wasted time** - Finding issues early saves hours of debugging later\n",
    "- **Guides feature engineering** - Understanding relationships helps create better features\n",
    "- **Sets realistic expectations** - We'll know what accuracy levels are possible\n",
    "\n",
    "---\n",
    "\n",
    "## üìã Dataset Overview\n",
    "\n",
    "**Source:** UCI Heart Disease Dataset (920 patient records from 4 medical centers)\n",
    "\n",
    "**Target Variable:** Heart disease severity (0-4)\n",
    "- **0** = No significant disease (< 50% artery blockage)\n",
    "- **1-4** = Progressively worse disease severity\n",
    "\n",
    "**Clinical Features (14):** Age, sex, chest pain type, blood pressure, cholesterol, ECG results, etc.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Setup: Import Libraries\n",
    "\n",
    "We'll import all the tools we need for analysis and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Libraries imported successfully!\n",
      "Pandas version: 2.2.2\n",
      "NumPy version: 1.26.4\n"
     ]
    }
   ],
   "source": [
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Statistical analysis\n",
    "from scipy import stats\n",
    "\n",
    "# Display settings for better visualization\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "pd.set_option('display.max_columns', None)  # Show all columns\n",
    "pd.set_option('display.max_rows', 100)      # Show up to 100 rows\n",
    "pd.set_option('display.precision', 3)        # 3 decimal places\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Load the Dataset\n",
    "\n",
    "**Important:** Make sure you've downloaded the dataset from Kaggle and placed it in `../data/raw/`\n",
    "\n",
    "The dataset should be named something like `heart.csv` or `heart_disease.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/raw/heart.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Load the dataset\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# MODIFY THIS PATH based on your actual filename\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../data/raw/heart.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m‚úÖ Dataset loaded successfully!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124müìä Dataset shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m rows √ó \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m columns\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[1;32m   1881\u001b[0m     f,\n\u001b[1;32m   1882\u001b[0m     mode,\n\u001b[1;32m   1883\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1884\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1885\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m   1886\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[1;32m   1887\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1888\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1889\u001b[0m )\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    874\u001b[0m             handle,\n\u001b[1;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m    876\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[1;32m    877\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[1;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    879\u001b[0m         )\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/raw/heart.csv'"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "# MODIFY THIS PATH based on your actual filename\n",
    "df = pd.read_csv('../data/raw/heart_disease_uci.csv')\n",
    "\n",
    "print(\"‚úÖ Dataset loaded successfully!\")\n",
    "print(f\"\\nüìä Dataset shape: {df.shape[0]} rows √ó {df.shape[1]} columns\")\n",
    "print(f\"   (We have {df.shape[0]} patient records with {df.shape[1]} features)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ First Look at the Data\n",
    "\n",
    "Let's see what the data actually looks like. The first few rows give us a quick sense of the structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first 10 rows\n",
    "print(\"üîç First 10 rows of the dataset:\")\n",
    "display(df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Understanding the Features\n",
    "\n",
    "Let's understand what each column means from a medical perspective:\n",
    "\n",
    "### üìã Feature Definitions:\n",
    "\n",
    "1. **age** - Age in years\n",
    "2. **sex** - Sex (1 = male, 0 = female)\n",
    "3. **cp** - Chest pain type:\n",
    "   - 0: Typical angina (chest pain from reduced blood flow)\n",
    "   - 1: Atypical angina\n",
    "   - 2: Non-anginal pain\n",
    "   - 3: Asymptomatic (no symptoms)\n",
    "4. **trestbps** - Resting blood pressure (mm Hg)\n",
    "5. **chol** - Serum cholesterol (mg/dl)\n",
    "6. **fbs** - Fasting blood sugar > 120 mg/dl (1 = true, 0 = false)\n",
    "7. **restecg** - Resting electrocardiographic results:\n",
    "   - 0: Normal\n",
    "   - 1: ST-T wave abnormality\n",
    "   - 2: Left ventricular hypertrophy\n",
    "8. **thalach** - Maximum heart rate achieved during exercise\n",
    "9. **exang** - Exercise induced angina (1 = yes, 0 = no)\n",
    "10. **oldpeak** - ST depression induced by exercise relative to rest\n",
    "11. **slope** - Slope of peak exercise ST segment:\n",
    "    - 0: Upsloping\n",
    "    - 1: Flat\n",
    "    - 2: Downsloping\n",
    "12. **ca** - Number of major vessels colored by fluoroscopy (0-3)\n",
    "13. **thal** - Thalassemia:\n",
    "    - 0: Normal\n",
    "    - 1: Fixed defect\n",
    "    - 2: Reversible defect\n",
    "14. **target** - Heart disease severity (0 = no disease, 1-4 = increasing severity)\n",
    "\n",
    "Let's check the column names and types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get basic information about the dataset\n",
    "print(\"üìä Dataset Information:\")\n",
    "print(\"=\" * 60)\n",
    "df.info()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"\\nüîç Column Names:\")\n",
    "print(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üí° What to Look For:\n",
    "- **Data types:** Are they correct? (integers for categorical, floats for continuous)\n",
    "- **Non-null counts:** Do we have missing values?\n",
    "- **Memory usage:** Is the dataset small enough to fit in memory?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Statistical Summary\n",
    "\n",
    "Statistical summaries help us understand:\n",
    "- **Range** of values (min, max)\n",
    "- **Central tendency** (mean, median)\n",
    "- **Spread** (standard deviation, quartiles)\n",
    "- **Potential outliers** (values far from mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìà Statistical Summary of Numerical Features:\")\n",
    "print(\"=\" * 80)\n",
    "display(df.describe().round(2))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"\\nüéØ What to Notice:\")\n",
    "print(\"- Age range:\", df['age'].min(), \"-\", df['age'].max(), \"years\")\n",
    "print(\"- Blood pressure range:\", df['trestbps'].min(), \"-\", df['trestbps'].max(), \"mm Hg\")\n",
    "print(\"- Cholesterol range:\", df['chol'].min(), \"-\", df['chol'].max(), \"mg/dl\")\n",
    "print(\"- Max heart rate range:\", df['thalach'].min(), \"-\", df['thalach'].max(), \"bpm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ü§î Questions to Ask:\n",
    "1. Do the ranges make medical sense?\n",
    "2. Are there any impossible values? (e.g., 0 mm Hg blood pressure)\n",
    "3. Are the means and medians similar? (If not, data might be skewed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Missing Values Analysis\n",
    "\n",
    "**Why this matters:** Missing data can:\n",
    "- Bias our model\n",
    "- Reduce predictive power\n",
    "- Lead to errors during training\n",
    "\n",
    "We need to know:\n",
    "1. **How many** values are missing?\n",
    "2. **Which columns** have missing data?\n",
    "3. **Is the missingness random** or systematic?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count missing values\n",
    "missing_counts = df.isnull().sum()\n",
    "missing_percent = (df.isnull().sum() / len(df)) * 100\n",
    "\n",
    "# Create a summary dataframe\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing_Count': missing_counts,\n",
    "    'Percentage': missing_percent\n",
    "})\n",
    "\n",
    "# Filter to only show columns with missing values\n",
    "missing_df = missing_df[missing_df['Missing_Count'] > 0].sort_values('Missing_Count', ascending=False)\n",
    "\n",
    "print(\"üîç Missing Values Analysis:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if len(missing_df) == 0:\n",
    "    print(\"‚úÖ Great news! No missing values detected!\")\n",
    "else:\n",
    "    display(missing_df)\n",
    "    \n",
    "    # Visualize missing data\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(missing_df.index, missing_df['Percentage'])\n",
    "    plt.xlabel('Features')\n",
    "    plt.ylabel('Percentage Missing (%)')\n",
    "    plt.title('Missing Data by Feature')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "print(f\"\\nüìä Total missing values: {df.isnull().sum().sum()}\")\n",
    "print(f\"üìä Overall completeness: {((1 - df.isnull().sum().sum() / (df.shape[0] * df.shape[1])) * 100):.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üí° Important Note on Missing Values:\n",
    "\n",
    "Sometimes values that look like valid numbers are actually **coded missing values**. For example:\n",
    "- A cholesterol of **0** is medically impossible (should be missing)\n",
    "- A blood pressure of **0** doesn't make sense\n",
    "\n",
    "Let's check for these \"disguised\" missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîç Checking for Suspicious Zero Values:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Check specific columns where 0 doesn't make sense\n",
    "suspicious_cols = ['trestbps', 'chol', 'thalach']\n",
    "\n",
    "for col in suspicious_cols:\n",
    "    if col in df.columns:\n",
    "        zero_count = (df[col] == 0).sum()\n",
    "        zero_percent = (zero_count / len(df)) * 100\n",
    "        print(f\"{col:15} : {zero_count:3} zero values ({zero_percent:.2f}%)\")\n",
    "        \n",
    "        if zero_count > 0:\n",
    "            print(f\"  ‚ö†Ô∏è  Warning: {col} has {zero_count} zero values (medically unlikely!)\")\n",
    "\n",
    "print(\"\\nüí° Note: We'll need to handle these in the preprocessing step!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Target Variable Analysis (THE MOST IMPORTANT PART!)\n",
    "\n",
    "### Why This is Critical:\n",
    "The target variable (what we're trying to predict) determines everything:\n",
    "- What type of problem this is (classification vs regression)\n",
    "- What metrics we'll use\n",
    "- What algorithms might work best\n",
    "- Whether we need special techniques (like SMOTE for imbalance)\n",
    "\n",
    "### What is Class Imbalance?\n",
    "**Class imbalance** occurs when we have many more examples of some classes than others.\n",
    "\n",
    "**Example:** If 90% of patients have no disease (class 0) and only 2% have severe disease (class 4):\n",
    "- A \"dumb\" model could just predict \"no disease\" for everyone and be 90% accurate!\n",
    "- But it would NEVER catch severe cases (the most important ones clinically)\n",
    "- This is why **accuracy alone is misleading** with imbalanced data\n",
    "\n",
    "Let's check our target distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming target column is named 'target' or 'num' (common in this dataset)\n",
    "# Adjust if your column name is different\n",
    "target_col = 'target' if 'target' in df.columns else 'num' if 'num' in df.columns else df.columns[-1]\n",
    "\n",
    "print(f\"üéØ Target Variable: '{target_col}'\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Count and percentage of each class\n",
    "target_counts = df[target_col].value_counts().sort_index()\n",
    "target_percent = (df[target_col].value_counts(normalize=True) * 100).sort_index()\n",
    "\n",
    "target_summary = pd.DataFrame({\n",
    "    'Count': target_counts,\n",
    "    'Percentage': target_percent.round(2)\n",
    "})\n",
    "\n",
    "print(\"\\nüìä Distribution of Heart Disease Severity:\")\n",
    "print()\n",
    "print(\"Class | Description                    | Count | Percentage\")\n",
    "print(\"-\" * 70)\n",
    "severity_labels = [\n",
    "    \"No disease (< 50% blockage)\",\n",
    "    \"Mild disease\",\n",
    "    \"Moderate disease\",\n",
    "    \"Severe disease\",\n",
    "    \"Very severe disease\"\n",
    "]\n",
    "\n",
    "for i in range(5):\n",
    "    if i in target_counts.index:\n",
    "        count = target_counts[i]\n",
    "        pct = target_percent[i]\n",
    "        label = severity_labels[i] if i < len(severity_labels) else f\"Class {i}\"\n",
    "        print(f\"  {i}   | {label:30} | {count:5} | {pct:6.2f}%\")\n",
    "    else:\n",
    "        print(f\"  {i}   | {severity_labels[i]:30} |     0 |   0.00%\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Bar plot\n",
    "axes[0].bar(target_counts.index, target_counts.values, color='skyblue', edgecolor='black')\n",
    "axes[0].set_xlabel('Severity Level', fontsize=12)\n",
    "axes[0].set_ylabel('Number of Patients', fontsize=12)\n",
    "axes[0].set_title('Distribution of Heart Disease Severity (Count)', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xticks(range(5))\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add count labels on bars\n",
    "for i, (idx, count) in enumerate(target_counts.items()):\n",
    "    axes[0].text(idx, count + 5, str(count), ha='center', fontweight='bold')\n",
    "\n",
    "# Pie chart\n",
    "colors = ['#90EE90', '#FFD700', '#FFA500', '#FF6347', '#DC143C']\n",
    "axes[1].pie(target_percent.values, labels=target_percent.index, autopct='%1.1f%%',\n",
    "            startangle=90, colors=colors[:len(target_percent)])\n",
    "axes[1].set_title('Proportion of Each Severity Level', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate imbalance ratio\n",
    "max_class = target_counts.max()\n",
    "min_class = target_counts.min()\n",
    "imbalance_ratio = max_class / min_class\n",
    "\n",
    "print(f\"\\n‚öñÔ∏è  Class Imbalance Analysis:\")\n",
    "print(f\"   Most common class: {target_counts.idxmax()} with {max_class} samples\")\n",
    "print(f\"   Least common class: {target_counts.idxmin()} with {min_class} samples\")\n",
    "print(f\"   Imbalance ratio: {imbalance_ratio:.2f}:1\")\n",
    "print()\n",
    "\n",
    "if imbalance_ratio > 3:\n",
    "    print(\"   ‚ö†Ô∏è  SIGNIFICANT IMBALANCE DETECTED!\")\n",
    "    print(\"   üìå We MUST use SMOTE or other balancing techniques\")\n",
    "    print(\"   üìå We CANNOT use simple accuracy as our metric\")\n",
    "    print(\"   üìå We MUST use stratified k-fold cross-validation\")\n",
    "else:\n",
    "    print(\"   ‚úÖ Classes are reasonably balanced\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéì Understanding the Results:\n",
    "\n",
    "**Imbalance Ratio** tells us how severe the imbalance is:\n",
    "- **< 2:1** - Mild imbalance (usually okay)\n",
    "- **2-3:1** - Moderate imbalance (consider balancing techniques)\n",
    "- **> 3:1** - Severe imbalance (MUST use balancing techniques)\n",
    "- **> 10:1** - Extreme imbalance (very challenging problem)\n",
    "\n",
    "**Why This Matters:**\n",
    "- Clinically, missing a severe case (false negative) is much worse than a false alarm\n",
    "- We want HIGH SENSITIVITY (ability to detect disease) even if it means more false alarms\n",
    "- This is why we'll use **weighted F1-score** instead of accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ Feature Distribution Analysis\n",
    "\n",
    "Understanding how each feature is distributed helps us:\n",
    "1. **Detect outliers** - Values that are suspiciously far from normal\n",
    "2. **Understand skewness** - Is the data bell-shaped or lopsided?\n",
    "3. **Identify scaling needs** - Do we need normalization?\n",
    "4. **Choose appropriate imputation** - Mean for normal distributions, median for skewed\n",
    "\n",
    "### üìä Continuous Features:\n",
    "Let's look at the continuous (numeric) features first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify continuous features (typically those with more unique values)\n",
    "continuous_features = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak']\n",
    "\n",
    "# Filter to only include features that exist in the dataset\n",
    "continuous_features = [f for f in continuous_features if f in df.columns]\n",
    "\n",
    "print(f\"üìä Analyzing {len(continuous_features)} Continuous Features:\")\n",
    "print(continuous_features)\n",
    "print()\n",
    "\n",
    "# Create subplots for histograms\n",
    "n_features = len(continuous_features)\n",
    "n_cols = 3\n",
    "n_rows = (n_features + n_cols - 1) // n_cols\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, n_rows * 4))\n",
    "axes = axes.flatten() if n_rows > 1 else [axes]\n",
    "\n",
    "for idx, feature in enumerate(continuous_features):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Histogram with KDE (Kernel Density Estimate)\n",
    "    ax.hist(df[feature].dropna(), bins=30, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    ax.set_xlabel(feature, fontsize=11)\n",
    "    ax.set_ylabel('Frequency', fontsize=11)\n",
    "    ax.set_title(f'Distribution of {feature}', fontsize=12, fontweight='bold')\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add mean and median lines\n",
    "    mean_val = df[feature].mean()\n",
    "    median_val = df[feature].median()\n",
    "    ax.axvline(mean_val, color='red', linestyle='--', linewidth=2, label=f'Mean: {mean_val:.1f}')\n",
    "    ax.axvline(median_val, color='green', linestyle='--', linewidth=2, label=f'Median: {median_val:.1f}')\n",
    "    ax.legend()\n",
    "\n",
    "# Hide empty subplots\n",
    "for idx in range(len(continuous_features), len(axes)):\n",
    "    axes[idx].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéì How to Interpret Distributions:\n",
    "\n",
    "**Normal Distribution (Bell Curve):**\n",
    "- Mean ‚âà Median\n",
    "- Symmetric shape\n",
    "- Most data near center\n",
    "- **Example:** Height, many physiological measurements\n",
    "\n",
    "**Right-Skewed (Positively Skewed):**\n",
    "- Mean > Median\n",
    "- Long tail to the right\n",
    "- Most values on the left\n",
    "- **Example:** Income, cholesterol in some populations\n",
    "- **Action:** Consider log transformation or use median for imputation\n",
    "\n",
    "**Left-Skewed (Negatively Skewed):**\n",
    "- Mean < Median\n",
    "- Long tail to the left\n",
    "- Most values on the right\n",
    "- **Example:** Age at death, test scores\n",
    "\n",
    "**Bimodal (Two Peaks):**\n",
    "- Two distinct groups\n",
    "- **Example:** Could indicate different populations (e.g., healthy vs. sick)\n",
    "- **Action:** Might need to analyze groups separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical tests for normality and skewness\n",
    "print(\"üìä Statistical Analysis of Distributions:\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"{'Feature':<15} | {'Mean':<8} | {'Median':<8} | {'Std Dev':<8} | {'Skewness':<10} | Shape\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for feature in continuous_features:\n",
    "    mean = df[feature].mean()\n",
    "    median = df[feature].median()\n",
    "    std = df[feature].std()\n",
    "    skew = df[feature].skew()\n",
    "    \n",
    "    # Determine shape\n",
    "    if abs(skew) < 0.5:\n",
    "        shape = \"Fairly symmetric\"\n",
    "    elif skew > 0.5:\n",
    "        shape = \"Right-skewed ‚û°Ô∏è\"\n",
    "    else:\n",
    "        shape = \"Left-skewed ‚¨ÖÔ∏è\"\n",
    "    \n",
    "    print(f\"{feature:<15} | {mean:>7.2f} | {median:>7.2f} | {std:>7.2f} | {skew:>9.2f} | {shape}\")\n",
    "\n",
    "print(\"\\nüí° Interpretation:\")\n",
    "print(\"   ‚Ä¢ Skewness near 0 = symmetric (normal-like)\")\n",
    "print(\"   ‚Ä¢ Positive skewness = right tail (mean > median)\")\n",
    "print(\"   ‚Ä¢ Negative skewness = left tail (mean < median)\")\n",
    "print(\"   ‚Ä¢ |Skewness| > 1 = highly skewed (consider transformation)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìä Categorical Features:\n",
    "\n",
    "Now let's look at categorical features (sex, chest pain type, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify categorical features\n",
    "categorical_features = ['sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'ca', 'thal']\n",
    "categorical_features = [f for f in categorical_features if f in df.columns]\n",
    "\n",
    "print(f\"üìä Analyzing {len(categorical_features)} Categorical Features:\")\n",
    "print(categorical_features)\n",
    "print()\n",
    "\n",
    "# Create subplots for bar charts\n",
    "n_features = len(categorical_features)\n",
    "n_cols = 3\n",
    "n_rows = (n_features + n_cols - 1) // n_cols\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, n_rows * 4))\n",
    "axes = axes.flatten() if n_rows > 1 else [axes]\n",
    "\n",
    "for idx, feature in enumerate(categorical_features):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Count values\n",
    "    value_counts = df[feature].value_counts().sort_index()\n",
    "    \n",
    "    # Bar plot\n",
    "    ax.bar(value_counts.index, value_counts.values, color='coral', edgecolor='black', alpha=0.7)\n",
    "    ax.set_xlabel(feature, fontsize=11)\n",
    "    ax.set_ylabel('Count', fontsize=11)\n",
    "    ax.set_title(f'Distribution of {feature}', fontsize=12, fontweight='bold')\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add count labels\n",
    "    for i, (cat, count) in enumerate(value_counts.items()):\n",
    "        ax.text(cat, count + value_counts.max() * 0.01, str(count), \n",
    "                ha='center', fontweight='bold')\n",
    "\n",
    "# Hide empty subplots\n",
    "for idx in range(len(categorical_features), len(axes)):\n",
    "    axes[idx].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9Ô∏è‚É£ Outlier Detection\n",
    "\n",
    "### What are Outliers?\n",
    "**Outliers** are data points that are significantly different from other observations. They can be:\n",
    "1. **Valid extreme values** - Some people are just unusual (e.g., Olympic athletes)\n",
    "2. **Measurement errors** - Equipment malfunction or recording mistakes\n",
    "3. **Data entry errors** - Typos (e.g., age 900 instead of 90)\n",
    "\n",
    "### Why Care About Outliers?\n",
    "- Can drastically affect model training\n",
    "- Can skew statistical measures (mean, standard deviation)\n",
    "- Some algorithms (like SVM) are very sensitive to outliers\n",
    "\n",
    "### Detection Method: Box Plots\n",
    "We'll use **box plots** which show:\n",
    "- **Box:** 25th to 75th percentile (middle 50% of data)\n",
    "- **Line in box:** Median (50th percentile)\n",
    "- **Whiskers:** Data within 1.5 √ó IQR (Interquartile Range)\n",
    "- **Dots:** Outliers (beyond whiskers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plots for outlier detection\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, n_rows * 4))\n",
    "axes = axes.flatten() if n_rows > 1 else [axes]\n",
    "\n",
    "for idx, feature in enumerate(continuous_features):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Create box plot\n",
    "    bp = ax.boxplot(df[feature].dropna(), vert=True, patch_artist=True)\n",
    "    \n",
    "    # Color the box\n",
    "    for patch in bp['boxes']:\n",
    "        patch.set_facecolor('lightblue')\n",
    "    \n",
    "    ax.set_ylabel(feature, fontsize=11)\n",
    "    ax.set_title(f'Outlier Detection: {feature}', fontsize=12, fontweight='bold')\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Hide empty subplots\n",
    "for idx in range(len(continuous_features), len(axes)):\n",
    "    axes[idx].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical outlier detection using IQR method\n",
    "print(\"üîç Outlier Analysis Using IQR Method:\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"{'Feature':<15} | {'Q1':<8} | {'Q3':<8} | {'IQR':<8} | {'Lower':<8} | {'Upper':<8} | Outliers\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "outlier_summary = {}\n",
    "\n",
    "for feature in continuous_features:\n",
    "    data = df[feature].dropna()\n",
    "    \n",
    "    # Calculate quartiles and IQR\n",
    "    Q1 = data.quantile(0.25)\n",
    "    Q3 = data.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    \n",
    "    # Calculate bounds\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    # Find outliers\n",
    "    outliers = data[(data < lower_bound) | (data > upper_bound)]\n",
    "    n_outliers = len(outliers)\n",
    "    outlier_percent = (n_outliers / len(data)) * 100\n",
    "    \n",
    "    outlier_summary[feature] = {\n",
    "        'count': n_outliers,\n",
    "        'percent': outlier_percent,\n",
    "        'values': outliers.tolist()\n",
    "    }\n",
    "    \n",
    "    print(f\"{feature:<15} | {Q1:>7.1f} | {Q3:>7.1f} | {IQR:>7.1f} | {lower_bound:>7.1f} | {upper_bound:>7.1f} | {n_outliers} ({outlier_percent:.1f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"\\nüí° IQR Method Explanation:\")\n",
    "print(\"   ‚Ä¢ IQR = Q3 - Q1 (spread of middle 50% of data)\")\n",
    "print(\"   ‚Ä¢ Lower bound = Q1 - 1.5√óIQR\")\n",
    "print(\"   ‚Ä¢ Upper bound = Q3 + 1.5√óIQR\")\n",
    "print(\"   ‚Ä¢ Values outside these bounds are considered outliers\")\n",
    "print(\"\\n‚ö†Ô∏è  Note: Not all outliers are errors! Some may be valid extreme cases.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîü Correlation Analysis\n",
    "\n",
    "### What is Correlation?\n",
    "**Correlation** measures how two variables move together:\n",
    "- **+1**: Perfect positive correlation (when one ‚Üë, other ‚Üë)\n",
    "- **0**: No correlation (no relationship)\n",
    "- **-1**: Perfect negative correlation (when one ‚Üë, other ‚Üì)\n",
    "\n",
    "### Why This Matters:\n",
    "1. **Feature Selection:** Highly correlated features are redundant\n",
    "2. **Model Performance:** Multicollinearity can hurt some models (like linear regression)\n",
    "3. **Understanding Relationships:** Which features relate to the target?\n",
    "4. **Feature Engineering:** Create new features by combining correlated ones\n",
    "\n",
    "### Medical Context:\n",
    "We expect some correlations, like:\n",
    "- Age and max heart rate (negative - older = lower max HR)\n",
    "- Exercise-induced angina and oldpeak (positive - both indicate stress response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlation matrix\n",
    "correlation_matrix = df.corr()\n",
    "\n",
    "# Create heatmap\n",
    "plt.figure(figsize=(14, 12))\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm', \n",
    "            center=0, square=True, linewidths=1, cbar_kws={\"shrink\": 0.8})\n",
    "plt.title('Correlation Matrix of All Features', fontsize=16, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüé® Heatmap Color Guide:\")\n",
    "print(\"   üî¥ Red = Strong positive correlation (both variables move together)\")\n",
    "print(\"   üîµ Blue = Strong negative correlation (variables move in opposite directions)\")\n",
    "print(\"   ‚ö™ White = No correlation (no relationship)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find strongest correlations with target\n",
    "target_corr = correlation_matrix[target_col].sort_values(ascending=False)\n",
    "\n",
    "print(\"\\nüéØ Features Most Correlated with Target (Heart Disease Severity):\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Feature':<20} | {'Correlation':<12} | Interpretation\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for feature, corr in target_corr.items():\n",
    "    if feature != target_col:\n",
    "        if abs(corr) > 0.5:\n",
    "            strength = \"STRONG\"\n",
    "        elif abs(corr) > 0.3:\n",
    "            strength = \"MODERATE\"\n",
    "        elif abs(corr) > 0.1:\n",
    "            strength = \"WEAK\"\n",
    "        else:\n",
    "            strength = \"VERY WEAK\"\n",
    "        \n",
    "        direction = \"positive\" if corr > 0 else \"negative\"\n",
    "        print(f\"{feature:<20} | {corr:>11.3f} | {strength} {direction}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize top correlations with target\n",
    "top_features = target_corr.drop(target_col).head(10)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "colors = ['green' if x > 0 else 'red' for x in top_features.values]\n",
    "plt.barh(range(len(top_features)), top_features.values, color=colors, alpha=0.7, edgecolor='black')\n",
    "plt.yticks(range(len(top_features)), top_features.index)\n",
    "plt.xlabel('Correlation with Target', fontsize=12)\n",
    "plt.title('Top 10 Features Correlated with Heart Disease', fontsize=14, fontweight='bold')\n",
    "plt.axvline(x=0, color='black', linestyle='--', linewidth=1)\n",
    "plt.grid(axis='x', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Interpretation:\")\n",
    "print(\"   üü¢ Green bars: As feature increases, heart disease severity increases\")\n",
    "print(\"   üî¥ Red bars: As feature increases, heart disease severity decreases\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç Finding Multicollinearity:\n",
    "\n",
    "**Multicollinearity** = When two features are highly correlated with each other (not just the target)\n",
    "\n",
    "**Why it's a problem:**\n",
    "- Redundant information (one feature doesn't add much if we already have the other)\n",
    "- Can confuse some models about which feature is important\n",
    "- Increases model complexity unnecessarily\n",
    "\n",
    "**Rule of thumb:** If |correlation| > 0.8, features are too similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find pairs of highly correlated features\n",
    "print(\"üîç Detecting Multicollinearity (|correlation| > 0.7):\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "high_corr_pairs = []\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i+1, len(correlation_matrix.columns)):\n",
    "        feat1 = correlation_matrix.columns[i]\n",
    "        feat2 = correlation_matrix.columns[j]\n",
    "        corr_value = correlation_matrix.iloc[i, j]\n",
    "        \n",
    "        if abs(corr_value) > 0.7 and feat1 != target_col and feat2 != target_col:\n",
    "            high_corr_pairs.append((feat1, feat2, corr_value))\n",
    "            print(f\"   {feat1} <-> {feat2}: {corr_value:.3f}\")\n",
    "\n",
    "if len(high_corr_pairs) == 0:\n",
    "    print(\"   ‚úÖ No severe multicollinearity detected!\")\n",
    "else:\n",
    "    print(f\"\\n   ‚ö†Ô∏è  Found {len(high_corr_pairs)} pairs of highly correlated features\")\n",
    "    print(\"   üí° Consider removing one feature from each pair during preprocessing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£1Ô∏è‚É£ Feature Relationships by Target Class\n",
    "\n",
    "Now let's see how features behave differently across severity levels. This helps us understand:\n",
    "- Which features truly separate classes\n",
    "- Whether patterns make medical sense\n",
    "- Which features will be most useful for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plots grouped by target class\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "selected_features = continuous_features[:6]  # Take first 6 for visualization\n",
    "\n",
    "for idx, feature in enumerate(selected_features):\n",
    "    if idx < len(axes):\n",
    "        ax = axes[idx]\n",
    "        \n",
    "        # Create grouped box plot\n",
    "        df.boxplot(column=feature, by=target_col, ax=ax)\n",
    "        ax.set_xlabel('Severity Level', fontsize=11)\n",
    "        ax.set_ylabel(feature, fontsize=11)\n",
    "        ax.set_title(f'{feature} by Heart Disease Severity', fontsize=12, fontweight='bold')\n",
    "        ax.get_figure().suptitle('')  # Remove default title\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° What to Look For:\")\n",
    "print(\"   ‚Ä¢ Clear separation between boxes = feature is discriminative\")\n",
    "print(\"   ‚Ä¢ Overlapping boxes = feature may not help distinguish classes\")\n",
    "print(\"   ‚Ä¢ Trend across severity levels = feature has predictive relationship\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£2Ô∏è‚É£ Pair Plots for Key Features\n",
    "\n",
    "**Pair plots** show relationships between multiple features at once. Each cell shows:\n",
    "- **Diagonal:** Distribution of single feature\n",
    "- **Off-diagonal:** Scatter plot of two features, colored by target class\n",
    "\n",
    "This helps us see:\n",
    "- Clustering patterns\n",
    "- Whether classes are separable\n",
    "- Non-linear relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select top features for pair plot (based on correlation with target)\n",
    "top_features_for_pairplot = target_corr.drop(target_col).abs().sort_values(ascending=False).head(4).index.tolist()\n",
    "features_to_plot = top_features_for_pairplot + [target_col]\n",
    "\n",
    "print(f\"üìä Creating pair plot for top {len(top_features_for_pairplot)} features...\")\n",
    "print(f\"Features: {top_features_for_pairplot}\")\n",
    "\n",
    "# Create pair plot\n",
    "pair_plot = sns.pairplot(df[features_to_plot], hue=target_col, palette='Set2', \n",
    "                         diag_kind='kde', plot_kws={'alpha': 0.6, 's': 80})\n",
    "pair_plot.fig.suptitle('Pair Plot of Top Features by Disease Severity', \n",
    "                       fontsize=16, fontweight='bold', y=1.01)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° How to Read This:\")\n",
    "print(\"   ‚Ä¢ Colors represent different severity levels\")\n",
    "print(\"   ‚Ä¢ Look for clear color separation = classes are distinguishable\")\n",
    "print(\"   ‚Ä¢ Diagonal shows feature distributions by class\")\n",
    "print(\"   ‚Ä¢ Off-diagonal shows feature relationships\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£3Ô∏è‚É£ Summary and Key Findings\n",
    "\n",
    "Let's summarize what we've learned from this EDA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"üéØ EXPLORATORY DATA ANALYSIS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nüìä DATASET OVERVIEW:\")\n",
    "print(f\"   ‚Ä¢ Total samples: {df.shape[0]}\")\n",
    "print(f\"   ‚Ä¢ Total features: {df.shape[1]} (including target)\")\n",
    "print(f\"   ‚Ä¢ Continuous features: {len(continuous_features)}\")\n",
    "print(f\"   ‚Ä¢ Categorical features: {len(categorical_features)}\")\n",
    "\n",
    "print(\"\\nüîç DATA QUALITY:\")\n",
    "total_missing = df.isnull().sum().sum()\n",
    "print(f\"   ‚Ä¢ Missing values: {total_missing} ({(total_missing / (df.shape[0] * df.shape[1]) * 100):.2f}%)\")\n",
    "if total_missing > 0:\n",
    "    print(f\"   ‚ö†Ô∏è  Action needed: Implement imputation strategy\")\n",
    "else:\n",
    "    print(f\"   ‚úÖ No missing values detected\")\n",
    "\n",
    "# Check for zero values in key features\n",
    "zero_issues = []\n",
    "for col in ['trestbps', 'chol', 'thalach']:\n",
    "    if col in df.columns:\n",
    "        zeros = (df[col] == 0).sum()\n",
    "        if zeros > 0:\n",
    "            zero_issues.append(f\"{col}: {zeros} zeros\")\n",
    "\n",
    "if zero_issues:\n",
    "    print(f\"   ‚ö†Ô∏è  Suspicious zero values found:\")\n",
    "    for issue in zero_issues:\n",
    "        print(f\"      - {issue}\")\n",
    "\n",
    "print(\"\\n‚öñÔ∏è  CLASS BALANCE:\")\n",
    "for i, count in target_counts.items():\n",
    "    pct = (count / len(df)) * 100\n",
    "    print(f\"   ‚Ä¢ Class {i}: {count} samples ({pct:.1f}%)\")\n",
    "\n",
    "if imbalance_ratio > 3:\n",
    "    print(f\"   ‚ö†Ô∏è  Severe imbalance (ratio: {imbalance_ratio:.1f}:1)\")\n",
    "    print(f\"   üìå MUST apply SMOTE or other balancing techniques\")\n",
    "\n",
    "print(\"\\nüìà KEY CORRELATIONS WITH TARGET:\")\n",
    "top_3_corr = target_corr.drop(target_col).abs().sort_values(ascending=False).head(3)\n",
    "for feature, corr in top_3_corr.items():\n",
    "    direction = \"‚Üë\" if target_corr[feature] > 0 else \"‚Üì\"\n",
    "    print(f\"   ‚Ä¢ {feature}: {abs(corr):.3f} {direction}\")\n",
    "\n",
    "print(\"\\nüîÑ MULTICOLLINEARITY:\")\n",
    "if len(high_corr_pairs) > 0:\n",
    "    print(f\"   ‚ö†Ô∏è  {len(high_corr_pairs)} pairs of highly correlated features\")\n",
    "    print(f\"   üìå Consider feature selection during preprocessing\")\n",
    "else:\n",
    "    print(f\"   ‚úÖ No severe multicollinearity issues\")\n",
    "\n",
    "print(\"\\nüéØ NEXT STEPS FOR PREPROCESSING:\")\n",
    "print(\"   1. ‚úÖ Handle missing/zero values (imputation strategy)\")\n",
    "print(\"   2. ‚úÖ Encode categorical variables (one-hot or label encoding)\")\n",
    "print(\"   3. ‚úÖ Scale/normalize features (StandardScaler or MinMaxScaler)\")\n",
    "print(\"   4. ‚úÖ Address class imbalance (SMOTE)\")\n",
    "print(\"   5. ‚úÖ Feature selection (remove highly correlated features if needed)\")\n",
    "print(\"   6. ‚úÖ Create train/test splits (stratified by target)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ EDA COMPLETE! Ready for preprocessing phase.\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéì Learning Objectives Achieved:\n",
    "\n",
    "By completing this EDA, you now understand:\n",
    "\n",
    "1. ‚úÖ **Why EDA is critical** - It's the foundation for all subsequent work\n",
    "2. ‚úÖ **Data quality assessment** - Missing values, outliers, errors\n",
    "3. ‚úÖ **Distribution analysis** - Normal, skewed, bimodal patterns\n",
    "4. ‚úÖ **Class imbalance** - Why it matters and how to detect it\n",
    "5. ‚úÖ **Correlation analysis** - Feature relationships and multicollinearity\n",
    "6. ‚úÖ **Feature behavior** - How features differ across target classes\n",
    "7. ‚úÖ **Medical context** - What the features mean clinically\n",
    "\n",
    "## üìö Key Concepts to Remember:\n",
    "\n",
    "- **EDA guides all preprocessing decisions**\n",
    "- **Class imbalance requires special handling (SMOTE, stratified CV)**\n",
    "- **Outliers aren't always errors - investigate before removing**\n",
    "- **High correlation ‚â† causation**\n",
    "- **Understanding your data > blindly applying algorithms**\n",
    "\n",
    "---\n",
    "\n",
    "## üíæ Save Findings\n",
    "\n",
    "Let's save our EDA findings for the team:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save summary statistics\n",
    "summary_stats = df.describe()\n",
    "summary_stats.to_csv('../results/eda_summary_statistics.csv')\n",
    "\n",
    "# Save correlation matrix\n",
    "correlation_matrix.to_csv('../results/correlation_matrix.csv')\n",
    "\n",
    "# Save key findings to a text file\n",
    "with open('../results/eda_findings.txt', 'w') as f:\n",
    "    f.write(\"EXPLORATORY DATA ANALYSIS FINDINGS\\n\")\n",
    "    f.write(\"=\" * 60 + \"\\n\\n\")\n",
    "    f.write(f\"Dataset Size: {df.shape[0]} samples, {df.shape[1]} features\\n\")\n",
    "    f.write(f\"Missing Values: {df.isnull().sum().sum()}\\n\")\n",
    "    f.write(f\"Class Imbalance Ratio: {imbalance_ratio:.2f}:1\\n\\n\")\n",
    "    f.write(\"Top 5 Features Correlated with Target:\\n\")\n",
    "    for i, (feature, corr) in enumerate(target_corr.drop(target_col).abs().sort_values(ascending=False).head(5).items(), 1):\n",
    "        f.write(f\"  {i}. {feature}: {corr:.3f}\\n\")\n",
    "\n",
    "print(\"\\n‚úÖ Results saved to ../results/\")\n",
    "print(\"   ‚Ä¢ eda_summary_statistics.csv\")\n",
    "print(\"   ‚Ä¢ correlation_matrix.csv\")\n",
    "print(\"   ‚Ä¢ eda_findings.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
